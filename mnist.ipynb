{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\").to_numpy()\n",
    "test = pd.read_csv(\"data/test.csv\").to_numpy()\n",
    "\n",
    "train[:,0]\n",
    "\n",
    "x_train, y_train = train[:,1:], train[:,0]\n",
    "x_test, y_test = test[:,1:], test[:,0]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        self.weights = np.random.uniform(-1,1,(out_features, in_features)) - 0.5\n",
    "        self.bias = np.random.uniform(-1,1,(out_features, 1)) - 0.5\n",
    "    \n",
    "    def linear(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.weights.dot(x) + self.bias\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self) -> None:\n",
    "        self.l1 = Linear(784, 15)\n",
    "        self.l2 = Linear(15, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"x = (n x 784)\"\n",
    "        x = x.T # (n x 784) --> (784 x n)\n",
    "        self.z1 = self.l1.linear(x) # (784 x n) --> (20 x n)\n",
    "        self.a1 = self.ReLU(self.z1)\n",
    "        self.z2 = self.l2.linear(self.a1) # (20 x n) --> (10 x n)\n",
    "        self.a2 = self.softmax(self.z2) # (n x 10)\n",
    "        return self.a2\n",
    "\n",
    "    def backwards(self, y, x):\n",
    "        m = y.size\n",
    "        y = self.one_hot(y)\n",
    "        dZ2 = self.a2 - y.T\n",
    "        print(dZ2, self.a1.T)\n",
    "        print(dZ2.dot(self.a1.T))\n",
    "        # dW2 = 1 / m * dZ2.dot(self.a1.T)\n",
    "        # db2 = 1 / m * np.sum(dZ2, 1)\n",
    "        # dZ1 = self.l2.weights.T.dot(dZ2) * self.deriv_ReLU(self.z1)\n",
    "        # dW1 = 1 / m * dZ1.dot(x)\n",
    "        # db1 = 1 / m * np.sum(dZ1, 1)\n",
    "        return #dW1, db2, dW2, db2\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot(y):\n",
    "        enc_y = np.zeros((y.size, y.max()+1))\n",
    "        enc_y[np.arange(y.size), y] = 1\n",
    "        return enc_y\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def deriv_ReLU(Z):\n",
    "        return Z > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        \"\"\"This returns the row-wise softmax of a numpy array\"\"\"\n",
    "        # stabilized = x - np.max(x, axis=0)\n",
    "        e_x = np.exp(x)\n",
    "        x = e_x / np.sum(e_x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.41006910e-06, 2.14481809e-06, 1.53121514e-06, 2.28343590e-06,\n",
       "       3.79404268e-06, 2.73866237e-06, 4.06158901e-06, 8.81143157e-07,\n",
       "       8.63109242e-07, 1.10143912e-06])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "y_pred = model.forward(x_train)\n",
    "y_pred[:,0]\n",
    "# model.z1[:,0]\n",
    "# model.backwards(y_train, x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550872.6430465538"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "    loss = -np.sum(y*np.log(y_pred.T))\n",
    "    return loss\n",
    "y = model.one_hot(y_train)\n",
    "cross_entropy(y, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(train)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(Z, 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "    \n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(get_accuracy(predictions, Y))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nanograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnanograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m MLP\n",
      "File \u001b[0;32m~/dev/neural-net-numpy/nanograd/nn.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Value\n\u001b[1;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mModule\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mzero_grad\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "from nanograd.nn import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(3, [4,4,1])\n",
    "\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0\n",
      "10 0.0\n",
      "20 0.0\n",
      "30 0.0\n",
      "40 0.0\n",
      "50 0.0\n",
      "60 0.0\n",
      "70 0.0\n",
      "80 0.0\n",
      "90 0.0\n",
      "100 0.0\n",
      "110 0.0\n"
     ]
    }
   ],
   "source": [
    "for k in range(120):\n",
    "    # forward pass\n",
    "    y_pred = [n(x) for x in xs]\n",
    "    loss = sum((y_out - y_gt)**2 for y_gt, y_out in zip(ys, y_pred))\n",
    "\n",
    "    # backward pass\n",
    "    n.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.03 * p.grad\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        print(k, loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0, grad=0.0),\n",
       " Value(data=-1.0, grad=0.0),\n",
       " Value(data=-1.0, grad=0.0),\n",
       " Value(data=1.0, grad=0.0)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            #https://en.wikipedia.org/wiki/Hyperbolic_functions#Derivatives\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir':'LR'})\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any values in the graph, create a rectangular record node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # if this value is a result of some operation, create an op node for it \n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.644302330276453\n",
      "10 0.12789674936858736\n",
      "20 0.02669108364639066\n",
      "30 0.002716197150222608\n",
      "40 0.00026326915684857726\n",
      "50 2.38239795562712e-05\n",
      "60 2.0954969835986387e-06\n",
      "70 1.827411707068524e-07\n",
      "80 1.5895734839687593e-08\n",
      "90 1.3816510721278513e-09\n",
      "100 1.200659505247582e-10\n",
      "110 1.0433090182590326e-11\n",
      "120 9.065623759111399e-13\n",
      "130 7.87734658627935e-14\n",
      "140 6.844811463879642e-15\n",
      "150 5.947614628229596e-16\n",
      "160 5.168018518533631e-17\n",
      "170 4.490610343309173e-18\n",
      "180 3.9019945862954174e-19\n",
      "190 3.3905337950954747e-20\n",
      "200 2.9460908076470287e-21\n",
      "210 2.559953789497077e-22\n",
      "220 2.2245557212727158e-23\n",
      "230 1.932671019667334e-24\n",
      "240 1.6792024796633683e-25\n",
      "250 1.4636118479066398e-26\n",
      "260 1.2758469287269017e-27\n",
      "270 1.1312758418935072e-28\n",
      "280 9.577264427448846e-30\n",
      "290 1.836566794967668e-30\n"
     ]
    }
   ],
   "source": [
    "n = MLP(3, [10,4,1])\n",
    "\n",
    "for k in range(300):\n",
    "    # forward pass\n",
    "    y_pred = [n(x) for x in xs]\n",
    "    loss = sum((y_out - y_gt)**2 for y_gt, y_out in zip(ys, y_pred))\n",
    "\n",
    "    # backward pass\n",
    "    n.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.03 * p.grad\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        print(k, loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0000000000000007, grad=1.3322676295501878e-15),\n",
       " Value(data=-0.9999999999999999, grad=2.220446049250313e-16),\n",
       " Value(data=-0.9999999999999996, grad=8.881784197001252e-16),\n",
       " Value(data=0.9999999999999996, grad=-8.881784197001252e-16)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Value(data=2.9400018210573545e-05, grad=1),\n",
       " [Value(data=0.9965012300542382, grad=-0.00699753989152363),\n",
       "  Value(data=-0.9979128801676643, grad=0.004174239664671342),\n",
       "  Value(data=-0.9992508893611005, grad=0.0014982212777989723),\n",
       "  Value(data=0.9965012300542382, grad=-0.00699753989152363)])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # forward pass\n",
    "y_pred = [n(x) for x in xs]\n",
    "loss = sum((y_out - y_gt)**2 for y_gt, y_out in zip(ys, y_pred))\n",
    "\n",
    "# backward pass\n",
    "n.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# update\n",
    "for p in n.parameters():\n",
    "    p.data += -0.05 * p.grad\n",
    "\n",
    "loss, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=3.063175324560093, grad=0),\n",
       " Value(data=-5.174558671545837, grad=0),\n",
       " Value(data=0.8995198412257644, grad=0),\n",
       " Value(data=0.7388069500948937, grad=0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9952375350608573, label=),\n",
       " Value(data=0.3481276520024903, label=),\n",
       " Value(data=-0.34357154982839777, label=),\n",
       " Value(data=0.25998945495712955, label=),\n",
       " Value(data=0.18451892991351748, label=),\n",
       " Value(data=-0.23510638454343002, label=),\n",
       " Value(data=-0.4906381666045556, label=),\n",
       " Value(data=-0.7181546648417907, label=)]"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a064d5beaecbac7435ae4dff5ea43f9741980359ab24bf8bd1116605e7f16270"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
